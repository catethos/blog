{
  
    
        "post0": {
            "title": "How to Estimate The Parameters of ODE using Bayesian Inference",
            "content": "We can easily solve an ordinary differential equation given the parameters and boundary conditions. For example, given the SIR model as described below . function sir!(du, u, p, t) S, I, R = u β, γ = p N = S + I + R a = β * S * I / N b = γ * I du[1] = -a du[2] = a - b du[3] = b return nothing end . sir! (generic function with 1 method) . and its parameters, timeframe and boundary condition, . tmax = 50.0 tspan = (0.0,tmax) obstimes = 1.0:1.0:tmax u0 = [990.0,10.0,0.0] # S,I.R p = [0.5,0.25]; # β,γ . we can solve it and get the usual curves. . prob_ode = ODEProblem(sir!,u0,tspan,p); sol_ode = solve(prob_ode, Tsit5(), saveat = 1.0); . plot(sol_ode; label=[&quot;S(t)&quot; &quot;I(t)&quot; &quot;R(t)&quot;], legend=:outertopright) . Bayesian Estimation . But what if we only has the noisy measurement of the infected number. Can we recover the parameters used? Let&#39;s extract the number of infected and add some Poisson noise. . sir_newcases = abs.(diff(sol_ode(0:40; idxs=1).u)) noisy_newcases = rand.(Poisson.(sir_newcases)); . bar(noisy_newcases; label=&quot;observed&quot;) plot!(sir_newcases; label=&quot;model&quot;,marker=:circle) . We can construct a bayesian model using Turing.jl . @model function sir_model(t, y, ::Type{T}=Float64) where {T} # sample the parameters: uniform priors for `i₀`, `β` and `γ` i₀ ~ Uniform(0.0, 1.0) β ~ Uniform(0.0, 1.0) γ ~ Uniform(0.0, 1.0) # simulate the SIR model and save the number of infected # individuals at times `t` I₀ = 1_000 * i₀ prob = ODEProblem{true}(sir!, T[1_000 - I₀, I₀, 0], (0.0, last(t)), (; β, γ)) sol = solve(prob, Tsit5(); saveat=t, save_idxs=1) # ensure that the simulation was successful if sol.retcode !== :Success Turing.@addlogprob! -Inf else # compute new cases cases = map(abs, diff(Array(sol))) # noisy observations for i in 1:length(y) y[i] ~ Poisson(cases[i]) end end return (; i₀, β, γ, y) end; . And use MCMC to estimate the parameters . sir_nuts = sample( sir_model(0:40, noisy_newcases), NUTS(0.65), MCMCThreads(), 1_000, 3; thinning=10, discard_initial=100, ) . ┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel └ @ AbstractMCMC /home/catethos/.julia/packages/AbstractMCMC/0eT8o/src/sample.jl:291 ┌ Info: Found initial step size │ ϵ = 0.05 └ @ Turing.Inference /home/catethos/.julia/packages/Turing/uMoX1/src/inference/hmc.jl:188 ┌ Warning: The current proposal will be rejected due to numerical error(s). │ isfinite.((θ, r, ℓπ, ℓκ)) = (true, false, false, false) └ @ AdvancedHMC /home/catethos/.julia/packages/AdvancedHMC/51xgc/src/hamiltonian.jl:47 ┌ Warning: The current proposal will be rejected due to numerical error(s). │ isfinite.((θ, r, ℓπ, ℓκ)) = (true, false, false, false) └ @ AdvancedHMC /home/catethos/.julia/packages/AdvancedHMC/51xgc/src/hamiltonian.jl:47 ┌ Info: Found initial step size │ ϵ = 0.2 └ @ Turing.Inference /home/catethos/.julia/packages/Turing/uMoX1/src/inference/hmc.jl:188 ┌ Warning: The current proposal will be rejected due to numerical error(s). │ isfinite.((θ, r, ℓπ, ℓκ)) = (true, false, false, false) └ @ AdvancedHMC /home/catethos/.julia/packages/AdvancedHMC/51xgc/src/hamiltonian.jl:47 ┌ Info: Found initial step size │ ϵ = 0.025 └ @ Turing.Inference /home/catethos/.julia/packages/Turing/uMoX1/src/inference/hmc.jl:188 ┌ Warning: The current proposal will be rejected due to numerical error(s). │ isfinite.((θ, r, ℓπ, ℓκ)) = (true, false, false, false) └ @ AdvancedHMC /home/catethos/.julia/packages/AdvancedHMC/51xgc/src/hamiltonian.jl:47 Sampling (1 threads): 100%|█████████████████████████████| Time: 0:00:07 . Chains MCMC chain (1000×15×3 Array{Float64, 3}): Iterations = 101:10:10091 Number of chains = 3 Samples per chain = 1000 Wall duration = 16.11 seconds Compute duration = 15.92 seconds parameters = i₀, β, γ internals = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size Summary Statistics parameters mean std naive_se mcse ess rhat ⋯ Symbol Float64 Float64 Float64 Float64 Float64 Float64 ⋯ i₀ 0.0092 0.0018 0.0000 0.0000 2936.9917 0.9996 ⋯ β 0.5000 0.0346 0.0006 0.0007 2915.3128 0.9999 ⋯ γ 0.2437 0.0297 0.0005 0.0006 2864.2599 1.0004 ⋯ 1 column omitted Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% Symbol Float64 Float64 Float64 Float64 Float64 i₀ 0.0063 0.0080 0.0090 0.0103 0.0132 β 0.4304 0.4770 0.5005 0.5233 0.5657 γ 0.1843 0.2245 0.2436 0.2639 0.3010 . And we can see that mean of the transimission rate is 0.5 and the recovery rate is 0.2437, which is very close to the values we used initially. . plot(sir_nuts) .",
            "url": "https://catethos.github.io/blog/probability/ode/bayesian/2022/05/09/ODE.html",
            "relUrl": "/probability/ode/bayesian/2022/05/09/ODE.html",
            "date": " • May 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "The Two Children Problem",
            "content": "Problem . Mr. Jones has two children. The older child is a girl. What is the probability that both children are girls? | Mr. Smith has two children. At least one of them is a boy. What is the probability that both children are boys? | . Using Python to Represent Probability . class Event: def __init__(self, pred, condition=None): self.pred = pred self.condition = condition def __or__(self, pred): return Event(self.pred, pred.pred) . class Probability: def __init__(self, sample_space): self.sample_space = sample_space def __call__(self, event): if event.condition: num = len({x for x in self.sample_space if event.pred(x) and event.condition(x)}) den = len({x for x in self.sample_space if event.condition(x)}) return num / den else: num = len({x for x in self.sample_space if event.pred(x)}) den = len(self.sample_space) return num/den . Examples . Before solving the two children problem, let&#39;s see of our code can solve the dice problem that we encounter in every introuction to probability course. . Example 1 . Rolling a fair dice with 6 faces, what is the probability of getting a even number ? . sample_space = {1,2,3,4,5,6} p = Probability(sample_space) # define the event of getting an even number is_even = Event(lambda x : x%2 == 0) # the probability of getting even number p(is_even) . 0.5 . Example 2 . (We can also look at conditional probability) What is the probability of getting an even number given the result is greater than 3 . greater_3 = Event(lambda x : x&gt;3) p(is_even | greater_3) . 0.6666666666666666 . Solving the real problem . Let&#39;s denote the gender by either the letter &#39;G&#39;(girl) or &#39;B&#39;(boy), and hence &#39;GB&#39; means the elder child is a girl and the younger is a boy. . sample_space = {&quot;GG&quot;,&quot;GB&quot;,&quot;BG&quot;,&quot;BB&quot;} p = Probability(sample_space) . older_is_boy = Event(lambda x: x[0] == &quot;B&quot;) at_least_one_boy = Event(lambda x: &quot;B&quot; in x) both_boy = Event(lambda x: x.count(&quot;B&quot;) == 2) . p(both_boy | older_is_boy) . 0.5 . p(both_boy | at_least_one_boy) . 0.3333333333333333 .",
            "url": "https://catethos.github.io/blog/probability/2022/05/02/paradox.html",
            "relUrl": "/probability/2022/05/02/paradox.html",
            "date": " • May 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "$n$ ways of looking at linear regression ($n$=1)",
            "content": "Problem Setup . Suppose that we want to predict salary based on age, education, gender, job, etc, the common way to set up this problem is to represent the $n$ data points with $m$ features as a matrix: . $$ X = begin{pmatrix} x_{1,1} &amp; x_{1,2} &amp; cdots &amp; x_{1,m} x_{2,1} &amp; x_{2,2} &amp; cdots &amp; x_{2,m} vdots &amp; vdots &amp; ddots &amp; vdots x_{n,1} &amp; x_{n,2} &amp; cdots &amp; x_{n,m} end{pmatrix} $$where $x_{i,1}, x_{i,2}, x_{i,3}, ...$ could be the age, education, job and other features for the $i$-person. . We could also represent the salaries for each person as a vector: . $$ y = begin{pmatrix} y_1 y_2 vdots y_n end{pmatrix} $$Thus linear regression can be thought of finding another vector . $$ beta = begin{pmatrix} beta_1 beta_2 vdots beta_m end{pmatrix} $$such that $$ X beta approx y $$ . We shall define the approximation $ approx$ rigorously later. . Some Linear Algebra . Given that $X in mathbb{R}^{n,m}$, the set $ {Xu| u in mathbb{R}^m }$ forms a linear subspace. . Example 1: Suppose that . $$ X= begin{pmatrix} 1 2 end{pmatrix} $$ then the set $ {Xu| u in mathbb{R} }$ is a straight line . Example 2: Suppose that . $$ X= begin{pmatrix} 1 &amp; 4 2 &amp; 5 3 &amp; 6 end{pmatrix} $$ then set $ {Xu| u in mathbb{R}^2 }$ is a plane . In general, the set $ {Xu| u in mathbb{R}^m }$ is a hyperplane. So if $y$ is on the hyperplane, then by definition there is a $ beta in mathbb{R}^m$ that can satisfy the equation $X beta=y$. . Now suppose $y$ is not on the hyperplane, next best thing we can do is to find a $ hat{y}$ on the hyperplabe that is closest to $y$, which is the orthogonal projection of $y$ onto the plane . . . By the definition of orthogonality, we know that . $$ X^T(y-X beta) = 0 $$and thus $$ beta = (X^TX)^{-1}X^Ty $$ . This is the analytical solution of our linear regression problem. . Coding Examples using Julia . Now let&#39;s try some concrete example to see if it actually works. Let&#39;s generate 100 random points using the real $ beta=0.2$ and some random noise, and figure out if we can recover the $ beta$ using only $X$ and $y$ . X = randn(100) β = 0.2 y = X*β + randn(100)/10 scatter(X, y) . Now we can calculate using the formula . (X&#39;*X)^-1*X&#39;*y . 0.19280849762611454 . 0.19 is really close to 2 (the real $ beta$) ! The same formula hold for higher dimension. We can&#39;t visualize it, but we can perform the calculation as usual. . X = randn(100,5) β = [0.1,0.2,0.3,0.4,0.5] y = X*β + randn(100)/10 (X&#39;*X)^-1*X&#39;*y . 5-element Vector{Float64}: 0.08658415160224578 0.19121983641861673 0.31556448265297393 0.4116056101763966 0.49374419922975754 . observe that $(0.08, 0.19, 0.32, 0.41, 0.49)$ is really close to $(0.1,0.2,0.3,0.4,0.5)$. .",
            "url": "https://catethos.github.io/blog/linear%20regression/theory/2022/05/01/nways1.html",
            "relUrl": "/linear%20regression/theory/2022/05/01/nways1.html",
            "date": " • May 1, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://catethos.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://catethos.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}